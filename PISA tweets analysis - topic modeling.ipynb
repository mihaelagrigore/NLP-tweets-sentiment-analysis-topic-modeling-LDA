{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00000-06fa82ba-af13-44d0-b86b-d4dd0729a467",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 25643,
    "execution_start": 1630257390714,
    "source_hash": "bc6311e1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start writing code here# numerical computation\n",
    "import numpy as np\n",
    "\n",
    "# data processing/manipulation\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import re\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "# stopwords, tokenizer, stemmer\n",
    "import nltk  \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Gensim\n",
    "!pip install gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary \n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "# spacy for lemmatization and additional stopwords\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# LDA plotting tools\n",
    "!pip install pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-51179086-406d-4ae0-9e94-b09977d41800",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2254,
    "execution_start": 1630257416369,
    "source_hash": "54e8c135",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading each dataset\n",
    "pisa_df = pd.read_csv('pisa_df_unique_gen.csv', lineterminator='\\n')\n",
    "\n",
    "pisa_df.rename(columns={\"text\": \"tweet\"}, inplace = True)\n",
    "\n",
    "pisa_df = pisa_df[pisa_df.lang == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00003-d4241453-7963-4999-b7fd-c007953dccd2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1630257418638,
    "source_hash": "bd0385c1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "keywords = ['pisa', 'pisa4development', 'pisaoecd', 'oecdpisa', 'pisa2003', 'pisa2006', 'pisa2009', 'pisa2012', 'pisa2015', 'pisa2018']\n",
    "unwanted = ['oecd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00004-5e9297f4-b663-4128-a767-7d121a710f51",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1630257418652,
    "source_hash": "de504eb0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Helper function that:\n",
    "    - removes accents\n",
    "    - removes stopwords\n",
    "    - removes punctuation\n",
    "    - remove all 1 and 2 letter 'words' that we create after automatic removal of the apostrophe character\n",
    "'''\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    # remove accents of text\n",
    "    text=unidecode(text)\n",
    "    \n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    text=re.sub(r'&amp;',' ',text) # remove ampersand \n",
    "    #text=re.sub(r'[^\\sa-zA-Z0-9@\\[\\]]',' ',text) # remove characters: punctuation and other special characters, except for: alphabet letters, numbers, '@' and '\\' \n",
    "    \n",
    "    text=re.sub(r'@[A-Za-z0-9]+','',text) #remove mentions of other twitter accounts (remove the whole user name)\n",
    "    text=re.sub(r'#','',text) # remove the hashtag symbol (but leaves the hashtag word)\n",
    "    text=re.sub(r'RT[\\s]+','',text) #remove retweet keyword\n",
    "    text=re.sub(r'rt[\\s]+','',text) #remove retweet keyword\n",
    "    text=re.sub(r'https?:\\/\\/\\S+','',text) # removes hyperlinks\n",
    "    text=re.sub(r'[^\\w]', ' ', text)\n",
    "\n",
    "    # remove stopwords\n",
    "    text=remove_stopwords(text)\n",
    "    \n",
    "    text = re.sub(r'@\\S+', \"\", text)    #remove all @name (mentions of other Twitter usernames) -> remove again mentioned of other usernames ?\n",
    "    text = re.sub(r\"http\\S+\", \"\", text) #remove all URLs (the whole URL, until the next ' ' is encountered) -> remove again all URL ?\n",
    "\n",
    "    '''\n",
    "    remove he mentions of Pisa\n",
    "    '''\n",
    "    for k in keywords :\n",
    "        to_remove = k\n",
    "        text = re.sub(to_remove, \" \", text)\n",
    "    \n",
    "    for w in unwanted :\n",
    "        to_remove = w\n",
    "        text = re.sub(to_remove, \" \", text)\n",
    "    \n",
    "    '''\n",
    "    remove any 1 or 2 letter entities that remain in a text after automatically removing the apostrophes \n",
    "    [d, f, i, m, s, t, u, y]\n",
    "    [el, en, la, ll, pa, ve]\n",
    "    '''\n",
    "    text = re.sub(r\"\\sd\\s\", \" \", text)\n",
    "    text = re.sub(r\"\\sf\\s\", \" \", text)\n",
    "    text = re.sub(r\"\\si\\s\", \" \", text)\n",
    "    text = re.sub(r\"\\sm\\s\", \" \", text)\n",
    "    text = re.sub(r\"\\ss\\s\", \" \", text)\n",
    "    text = re.sub(r\"\\st\\s\", \" \", text)\n",
    "    text = re.sub(r\"\\su\\s\", \" \", text)\n",
    "    text = re.sub(r\"\\sy\\s\", \" \", text)    \n",
    "    text = re.sub(r\"\\sel\\s\", \" \", text)\n",
    "    text = re.sub(r\"\\sen\\s\", \" \", text)\n",
    "    text = re.sub(r\"\\sla\\s\", \" \", text)\n",
    "    text = re.sub(r\"\\sll\\s\", \" \", text)\n",
    "    text = re.sub(r\"\\spa\\s\", \" \", text)    \n",
    "    text = re.sub(r\"\\sve\\s\", \" \", text)\n",
    "    \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00005-f94e3a81-60c0-4ecf-abbb-56d27dd66323",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1630257418661,
    "source_hash": "e38162d6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "        \n",
    "# Lemmatization function\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00006-905d11c3-2cc9-47c2-9a49-9a4e7f9bb208",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1630257418714,
    "source_hash": "844a9445",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize Tweets \n",
    "def tokenize_text(tweet):\n",
    "    filtered_tweet = []\n",
    "    words = word_tokenize(tweet) \n",
    "\n",
    "    for word in words:\n",
    "        filtered_tweet.append(word)\n",
    "                \n",
    "    return filtered_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00007-f91c401e-7a43-4019-8535-f724f664c1ed",
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     21
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7978,
    "execution_start": 1630257418715,
    "source_hash": "1269a14e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif you want to compare the original and \\nthe cleaned text to see what exactly was removed \\nuncomment both lines below\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning all trump and biden tweets by applying clean_text()\n",
    "pisa_df['clean_tweets'] = pisa_df['tweet'].apply(lambda x: clean_text(x))\n",
    "\n",
    "'''\n",
    "if you want to compare the original and \n",
    "the cleaned text to see what exactly was removed \n",
    "uncomment both lines below\n",
    "'''\n",
    "#pd.options.display.max_colwidth = 300\n",
    "#print(twitter_usa_df[['tweet', 'cleaned_tweet']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00008-bb996f45-beda-4426-ade0-d681823581bb",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 91384,
    "execution_start": 1630257426695,
    "source_hash": "6f6d733",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lemmatazing the  tweets by applying lemmatize_sentence()\n",
    "pisa_df['lemmat_tweets'] = pisa_df.clean_tweets.apply(lambda x: lemmatize_sentence(x))\n",
    "\n",
    "pisa_df['tokenized_tweets'] = pisa_df.lemmat_tweets.apply(lambda x: tokenize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00009-38453f18-d9f4-4015-baee-f7ed88ab843a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1630257518126,
    "source_hash": "2827a2c3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer( analyzer='word',\n",
    "                             min_df=3,# minimum required occurences of a word \n",
    "                             stop_words='english',# remove stop words\n",
    "                             lowercase=True,# convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',# num chars > 3\n",
    "                             max_features=5000,# max number of unique words\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00010-e598530c-dbf4-467e-be37-303a870b57f5",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 826,
    "execution_start": 1630257518127,
    "source_hash": "6900381e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_matrix = vectorizer.fit_transform(pisa_df.lemmat_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00011-e11a42c1-a363-41fb-8a23-eded4643f07b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 209843,
    "execution_start": 1630257518960,
    "source_hash": "97592b5b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA model for 14 topics took: 3.50\n"
     ]
    }
   ],
   "source": [
    "topics = 14\n",
    "\n",
    "lda_model = LatentDirichletAllocation(\n",
    "n_components=topics, # Number of topics\n",
    "learning_method='online',\n",
    "random_state=62,       \n",
    "n_jobs = -1  # Use all available CPUs\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "lda_output = lda_model.fit_transform(data_matrix)\n",
    "stop_time = time.time()\n",
    "\n",
    "print(f'Fitting LDA model for {topics} topics took: {(stop_time-start_time)/60:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00012-df13bb12-767d-41c4-a7d4-f392b5561646",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7087,
    "execution_start": 1630257728796,
    "source_hash": "d701b7b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "p = pyLDAvis.sklearn.prepare(lda_model, data_matrix, vectorizer, mds='tsne')\n",
    "pyLDAvis.save_html(p, 'lda_14topics_min3.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00015-0ab34edd-6a9f-4e4e-b0d0-2dd7db7fd3d3",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Predictive model for the new tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00017-25e7aa7e-3ec8-4109-a1dd-2a4df093e470",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4914,
    "execution_start": 1630257735881,
    "source_hash": "e8ffb2e1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_values = lda_model.transform(data_matrix)\n",
    "pisa_df['topic'] = topic_values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00017-b5f6c262-dca8-43d3-81ac-0e74f1213d6e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4028,
    "execution_start": 1630257740800,
    "source_hash": "dd9bff7a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pisa_df.to_csv('pisa_df_unique_gen_topic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00016-1d032f7c-4f33-4658-84e6-e1133e43fc3c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 25,
    "execution_start": 1630257744832,
    "source_hash": "f9900968",
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save('topic_values', topic_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00017-ed9a8cfe-5b74-4471-8fc4-f0e2c0b19d9d",
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     21
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 18,
    "execution_start": 1630257744862,
    "source_hash": "a723c926",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80839, 38)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pisa_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=914019af-b74b-46cd-bce4-57738580bf42' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "a382c91e-8a83-42f6-8380-577bad6a4e4e",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
